# ChatDKU Advising FAQ Evaluation with RAGAS  

This project evaluates the accuracy and relevance of responses generated by **ChatDKU Advising** using **RAGAS**. The evaluation is conducted by comparing ChatDKU Advising's answers to the official FAQ responses provided by the **Academic Advising Office at Duke Kunshan University (DKU)**.  

## About ChatDKU  
**ChatDKU** is a **RAG-based AI chatbot** designed to enhance campus interaction by providing academic advising and administrative support. The system integrates multi-source retrieval, query optimization, and context-aware prompt engineering to deliver high-quality responses.  

> âš ï¸ **Due to its potential future role as an official DKU resource, the complete project code remains private.** However, this evaluation project provides insight into its effectiveness.  

### ğŸ”¹ ChatDKU Introduction Video  
[æ ¡å›­å¤§æ¨¡å‹ChatDKUä¸Šçº¿æ˜†å±±æœå…‹å¤§å­¦ (Bilibili)](https://www.bilibili.com/video/BV1vw1sYmEvJ/?share_source=copy_web&vd_source=1a660fcaf4a30e9ca2312cb60f412a85)  

### ğŸ”¹ Live Demo  
[ChatDKU-Advising](http://chatdku.dukekunshan.edu.cn)  
(*Requires **DKU VPN** & **NetID Login** to access*)  

## Features  
- Uses `ragas.dataset_schema.SingleTurnSample` to structure FAQ data.  
- Evaluates generated responses using **RAGAS** metrics:  
  - **BLEU Score**: Measures n-gram overlap between generated and reference responses.  
  - **ROUGE Score**: Compares recall-based textual similarity.  
  - **Non-LLM String Similarity & Distance Measure**: Evaluates lexical similarity between responses.  

## Dataset  
The FAQ dataset consists of officially provided advising questions and answers, covering topics such as:  

- **Academic Honors**  
- **Academic Standing**  
- **CR/NC Grading**  
- **Course Load**  
- **Course Registration**  
- **Course Repeat**  
- **Course Withdrawal**  
- **Credits Transfer**  
- **Global Education**  
- **Graduation**  
- **Incomplete Grade**  
- **Leave of Absence**  
- **PE & NSPHST**  

## Usage  
Run the script to test **ChatDKU Advising**'s responses against reference answers and obtain performance metrics.  

## Dependencies  
- `ragas`  
- `asyncio`  

## Purpose  
This project **is part of my signature work and graduation project** at DKU. It aims to assess the reliability of **ChatDKU Advising** in providing accurate academic guidance, ensuring alignment with **DKUâ€™s official advising policies**.  

### Evaluation Summary of ChatDKU-Advising
# Average Scores Report  

## ğŸ“Š Overview  

This section provides visualizations of the evaluation results, including **average scores, radar charts, score distributions, and bar charts**.  

---

### ğŸ“Œ Average Scores Report  
<img width="271" alt="Average Scores Report" src="https://github.com/user-attachments/assets/e09112ab-5b4b-47ad-9fd8-2102b579cab4" />  

---

### ğŸ“Œ Corrected Radar Chart  
<img width="274" alt="Corrected Radar Chart" src="https://github.com/user-attachments/assets/8b841019-5c5a-4af7-a7d4-fa0b8a5c1eb8" />  

---

### ğŸ“Š Score Distributions & Bar Chart  

<table>
  <tr>
    <td align="center">
      <strong>Score Distributions Histogram</strong><br>
      <img width="410" alt="Score Distributions Histogram" src="https://github.com/user-attachments/assets/4d98dd52-d782-488e-84fc-2da729e8e668" />
    </td>
    <td align="center">
      <strong>Bar Chart of Average Scores</strong><br>
      <img width="468" alt="Bar Chart of Average Scores" src="https://github.com/user-attachments/assets/1fd5b2b1-b190-4b3e-ac01-de54ea5366c3" />
    </td>
  </tr>
</table>

ChatDKU was evaluated using 104 questions categorized into 13 domains from Duke Kunshan Universityâ€™s official FAQ documents. The systemâ€™s responses were compared against reference answers using three metrics: **Levenshtein** (textual similarity), **BLEU** (phrase-level precision), and **ROUGE** (content recall). Key findings include:
- **Overall Performance**: ChatDKU achieved strong results in Levenshtein (avg. 0.7877) and ROUGE (avg. 0.8111), indicating high structural and content-level alignment with official answers. However, BLEU scores (avg. 0.6018) highlighted inconsistencies in exact phrase matching.
- **Top Categories**: Courses like *Course Repeat* (Levenshtein: 0.9377) and *Credits Transfer* (ROUGE: 0.9600) excelled due to standardized responses.
- **Challenges**: Categories like *Leave of Absence* (Levenshtein: 0.6143) and *Graduation* (BLEU: 0.4739) showed lower scores, suggesting structural or content gaps.

### Strengths & Limitations
- **Strengths**: ChatDKU demonstrates robust retrieval capabilities and context-aware response generation, particularly for structured queries.
- **Limitations**: Phrase-level precision and handling open-ended questions remain areas for improvement. Traditional metrics like BLEU may understate the value of semantically accurate but paraphrased answers.

### Future Directions
Enhancing prompt engineering, expanding dataset coverage, and integrating advanced evaluation methods"""





