# ChatDKU Advising FAQ Evaluation with RAGAS

This project evaluates the accuracy and relevance of responses generated by ChatDKU Advising using RAGAS. The evaluation is conducted by comparing ChatDKU Advising's answers to the official FAQ responses provided by the Academic Advising Office at Duke Kunshan University.

## Features
- Uses `ragas.dataset_schema.SingleTurnSample` to structure FAQ data.
- Evaluates generated responses using various RAGAS metrics:
  - **BLEU Score**: Measures n-gram overlap between generated and reference responses.
  - **ROUGE Score**: Compares recall-based textual similarity.
  - **Non-LLM String Similarity & Distance Measure**: Evaluates lexical similarity between responses.

## Dataset
The FAQ dataset consists of officially provided advising questions and answers, covering topics such as:
Academic Honors    
Academic Standing     
CR/NC Grading       
Course Load        
Course Registration    
Course Repeat        
Course Withdrawal   
Credits Transfer     
Global Education   
Graduation          
Incomplete Grade      
Leave of Absence   
PE & NSPHST            

## Usage
Run the script to test ChatDKU Advising's responses against the reference answers and obtain performance metrics.

## Dependencies
- `ragas`
- `asyncio`

## Purpose
This project aims to assess the reliability of ChatDKU Advising in providing accurate academic guidance, ensuring alignment with DKUâ€™s official advising policies.
